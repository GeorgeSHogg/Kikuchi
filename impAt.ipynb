{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8908a772-6530-4cd9-b45b-4db776b427f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T11:59:44.282380Z",
     "iopub.status.busy": "2023-09-23T11:59:44.281792Z",
     "iopub.status.idle": "2023-09-23T11:59:44.287539Z",
     "shell.execute_reply": "2023-09-23T11:59:44.286548Z",
     "shell.execute_reply.started": "2023-09-23T11:59:44.282350Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch \n",
    "import torch.utils.data as tor_utils\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as torchmodels\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastai import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.core import DataLoaders as fast_dataloaders\n",
    "from scipy.ndimage import zoom\n",
    "import orix\n",
    "from orix.quaternion import Rotation, Symmetry\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from scipy.ndimage import median_filter\n",
    "import cv2\n",
    "import torch.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1224dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T11:59:44.291979Z",
     "iopub.status.busy": "2023-09-23T11:59:44.291750Z",
     "iopub.status.idle": "2023-09-23T11:59:44.308083Z",
     "shell.execute_reply": "2023-09-23T11:59:44.307478Z",
     "shell.execute_reply.started": "2023-09-23T11:59:44.291960Z"
    }
   },
   "outputs": [],
   "source": [
    "#data related functions\n",
    "\n",
    "\n",
    "def getMps(steel_type):\n",
    "    \"\"\"\n",
    "    A wrapper function to load the master pattern stored in steelH5s'\n",
    "\n",
    "    Args:\n",
    "        steel_type (str): The type of steel to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        mp (kikuchipy masterpattern): The loaded master pattern in default projection.\n",
    "        mp.as_lambert(kikuchipy masterpattern): The loaded master pattern in lambert projection.\n",
    "\n",
    "    \"\"\"\n",
    "    mp = kp.load(\"steelH5s/\" + steel_type + \".h5\")\n",
    "    return mp, mp.as_lambert()\n",
    "\n",
    "def quatBreakdown(fPath):\n",
    "    \"\"\"\n",
    "    A function to apply a regex search to extract the rotation quaternions from a filename.\n",
    "    \n",
    "    Args:\n",
    "        fPath (str): The filepath which is the be broken down.\n",
    "\n",
    "    Returns:\n",
    "        signleTarget (list of floats): A list of the numbers between underscores in the filepath.\n",
    "    \"\"\"\n",
    "    #create and apply the pattern which looks inbetween underscores and extracts the strings inbetween\n",
    "    pat = r'[^\\\\_]+(?=_)+.+[^_.jpeg]'  \n",
    "    pat = re.compile(pat)  \n",
    "    extracted = pat.search(str(fPath)).group()\n",
    "\n",
    "    #cast the extracted strings to floats\n",
    "    singleTarget = [float(lbl) for lbl in extracted.split(\"_\")[1: -1]]\n",
    "    return singleTarget\n",
    "\n",
    "def getNpsDataset(path, regexFunc):  \n",
    "\n",
    "    \"\"\"\n",
    "    A function to load all NPS files in a specified folder into a dataset and use the regexFunc to generate labels from filenames.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory path where NPS files are stored.\n",
    "        regexFunc (regex pattern): A function to apply regex operations on filenames to generate labels.\n",
    "\n",
    "    Returns:\n",
    "        NpsDataset: A dataset containing loaded NPS files and their labels.\n",
    "    \"\"\"\n",
    "\n",
    "    #get all files in a folder\n",
    "    files = os.listdir(path)\n",
    "    fileList = [path + f for f in files if os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "    #load all the files to an Nps dataset\n",
    "    npsDataset = NpsDataset(fileList, regexFunc)\n",
    "    return npsDataset\n",
    "\n",
    "def loadNpsFile(filePath):\n",
    "    \"\"\"\n",
    "    Load an NPS file.\n",
    "\n",
    "    Args:\n",
    "        filePath (str): Path to the NPS file.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Loaded data from the NPS file.\n",
    "    \"\"\"\n",
    "    return np.load(filePath)\n",
    "\n",
    "class NpsDataset(tor_utils.Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    A pytorch dataset class for loading NPS files. It also breaks down their filenames, using a regex function, to generate labels.\n",
    "    \n",
    "    Attributes:\n",
    "        fileList (list): List of paths to NPS files.\n",
    "        regexFunc (function): A function to apply regex operations on filenames to generate labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fileList, regexFunc):\n",
    "        #initialise with the file list of the nps files and the regex function for breakdown\n",
    "        self.fileList = fileList\n",
    "        self.regexFunc = regexFunc\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of items in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.fileList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \"\"\"\n",
    "        Get the data and label for the item at the specified index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the desired item.\n",
    "        \n",
    "        Returns:\n",
    "            torch tensor: Image data \n",
    "            torch tensor: The label with the regex function applied\n",
    "        \"\"\"\n",
    "\n",
    "        filePath = self.fileList[idx]\n",
    "        data = loadNpsFile(filePath)\n",
    "        return torch.tensor(data).unsqueeze(0), torch.tensor(self.regexFunc(filePath))\n",
    "\n",
    "    def subset(self, indices):\n",
    "\n",
    "        \"\"\"\n",
    "        Get a subset of items from the dataset based on the specified indices.\n",
    "        \n",
    "        Args:\n",
    "            indices (list): List of indices specifying the desired subset.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of items corresponding to the specified indices.\n",
    "        \"\"\"\n",
    "        return [self[i] for i in indices]\n",
    "\n",
    "def resize(pat, size):\n",
    "    \"\"\"\n",
    "    Resize a given image array to the specified size.\n",
    "    \n",
    "    Args:\n",
    "        pat (numpy array): Input image array to be resized.\n",
    "        size (int): Desired size for both width and height of the output image.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Resized image array.\n",
    "    \"\"\"\n",
    "\n",
    "    img = Image.fromarray(pat)\n",
    "    img = img.resize((size, size))\n",
    "    data = np.asarray(img)\n",
    "    return data\n",
    "\n",
    "def resize_batch(pat, size):\n",
    "    \"\"\"\n",
    "    Resize a batch of images using the zoom function to a given size.\n",
    "    \n",
    "    Args:\n",
    "        pat (numpy array): Input 3D array representing a batch of images (number_of_images, height, width).\n",
    "        size (int): Desired size for both width and height of each output image in the batch.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Resized batch of images.\n",
    "    \"\"\"\n",
    "    #calculate zoom factor needed\n",
    "    zoomFactors = (1, size / pat.shape[1], size / pat.shape[2])\n",
    "\n",
    "    resizedArray = zoom(pat, zoomFactors)\n",
    "    return resizedArray\n",
    "\n",
    "\n",
    "class SymQuatBreakdown:\n",
    "    \"\"\"\n",
    "    A class to find symmetrical points and return all distinguished pounts from a given quaternion\n",
    "    \n",
    "    Attributes:\n",
    "        sym: The given symmetry for quaternion breakdown.\n",
    "        singleTarget: A single target quaternion that needs to be broken down.\n",
    "        allTargetsData: Tensor containing the distinguished points of the symmetry breakdown.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sym):\n",
    "        self.sym = sym\n",
    "        self.singleTarget = None\n",
    "        self.allTargetsData = None\n",
    "    \n",
    "    def breakdown(self, singleTarget):\n",
    "        \"\"\"\n",
    "        Break down the given target quaternion into distinguished points based on the provided symmetry.\n",
    "        \n",
    "        Args:\n",
    "            singleTarget: The quaternion that needs to be broken down.\n",
    "        \n",
    "        Returns:\n",
    "            torch tensor: Tensor containing the distinguished points of the quaternion breakdown.\n",
    "        \"\"\"\n",
    "\n",
    "        #generate all distinguished points with similarity to the target\n",
    "        self.singleTarget = Symmetry(singleTarget)\n",
    "        self.allTargetsData = torch.tensor(orix.quaternion.symmetry.get_distinguished_points(self.sym, self.singleTarget).data).float()\n",
    "        \n",
    "        #fill out the tensor with zeros in order for all labels to have the same shape\n",
    "        numSlots = 96 - self.allTargetsData.size(0)\n",
    "        firstValue = self.allTargetsData[0, :]\n",
    "        filledData = firstValue.repeat(numSlots, 1)\n",
    "        self.allTargetsData = torch.cat((self.allTargetsData, filledData), dim=0)\n",
    "        return self.allTargetsData\n",
    "\n",
    "class greyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class to handle grayscale images and their corresponding labels. \n",
    "    Allows optional transformations to be applied to the images.\n",
    "    \n",
    "    Attributes:\n",
    "        data (torch tensor): Greyscale image data \n",
    "        labels (torch tensor: Labels corresponding to each image.\n",
    "        tmfs (list, optional): List of transformations to be applied on the images.\n",
    "        transform (torch transform, optional): A single transform composed of all the transforms in tmfs.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, tmfs=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tmfs = tmfs\n",
    "        self.transform = transforms.Compose(self.tmfs) if self.tmfs else None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of items in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve the image and its corresponding label at the specified index.\n",
    "        Applies transformations on the image if specified.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the desired item.\n",
    "        \n",
    "        Returns:\n",
    "            image (pytorch tensor): Transformed, if specified, greyscale image data.\n",
    "            label (pytorch tensor): Labels corresponding to the image.\n",
    "        \"\"\"\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "def closestQuaternion(reference, quaternions):\n",
    "    \"\"\"\n",
    "    Finds the quaternion closest to the reference quaternion based on dot product.\n",
    "    \n",
    "    Parameters:\n",
    "        reference (torch.Tensor): Reference quaternion.\n",
    "        quaternions (torch.Tensor): Tensor of quaternions to search through.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The closest quaternion to the reference.\n",
    "    \"\"\"\n",
    "    reference = reference.unsqueeze(0)\n",
    "    #Calculate dot products between the reference and each quaternion in the list\n",
    "    dots = torch.sum(reference * quaternions, dim = 1)\n",
    "    #Find the index of the maximum dot product\n",
    "    closestInd = np.argmax(dots)\n",
    "    return quaternions[closestInd]\n",
    "    \n",
    "class oneHotSymQuatBreakdown:\n",
    "    \"\"\"\n",
    "    A class to find symmetrical points and return all distinguished pounts from a given quaternion and include a portion of the tensor which represents the one hot encoding of the material type\n",
    "    \n",
    "    Attributes:\n",
    "        sym (kikuchipy phase): The given symmetry for quaternion breakdown.\n",
    "        singleTarget (torch tensor): A single target quaternion that needs to be broken down.\n",
    "        allTargetsData (torch tensor): Tensor containing the distinguished points of the symmetry breakdown.\n",
    "        maxSize (int): Maximum number of symmetrical points in all symmetries, will cast all outputs to have this shape\n",
    "    \"\"\"\n",
    "    def __init__(self, numMaterial, maxSize = 96):\n",
    "        self.numMaterial = numMaterial\n",
    "        self.maxSize = maxSize\n",
    "        \n",
    "    def breakdown(self, single_target, oneHotLoc, sym):\n",
    "        self.sym = sym\n",
    "\n",
    "        #Get symmetrical distinguished points for the target\n",
    "        self.singleTarget = Symmetry(single_target)\n",
    "        self.allTargetsData = torch.tensor(orix.quaternion.symmetry.get_distinguished_points(self.sym, self.singleTarget).data).float()\n",
    "        \n",
    "        # Extend the target data tensor to have maxSize rows\n",
    "        num_slots_to_fill = 96 - self.allTargetsData.size(0)\n",
    "        first_value = self.allTargetsData[0, :]\n",
    "        filled_data = first_value.repeat(num_slots_to_fill, 1)\n",
    "        self.allTargetsData = torch.cat((self.allTargetsData, filled_data), dim=0)\n",
    "        oneHot = torch.zeros((self.maxSize, self.numMaterial))\n",
    "        oneHot[:, oneHotLoc] = 1\n",
    "        return torch.cat([self.allTargetsData, oneHot], axis = 1)\n",
    "    \n",
    "def grainSplit(img, thresh):\n",
    "    \"\"\"\n",
    "    Splits the grains in an image based on differences in adjacent pixels' values.\n",
    "    \n",
    "    Args:\n",
    "        img (torch.Tensor): The input image, assumed to be in RGB format.\n",
    "        thresh (float): Threshold for determining grain boundaries. \n",
    "    \n",
    "    Returns:\n",
    "        split (torch.Tensor): A binary image indicating the splits between grains. Pixels marked `True` are on grain boundaries, while those marked `False` are not.\n",
    "    \"\"\"\n",
    "    # Calculate the difference between each pixel and the one below it\n",
    "    downDif = torch.sum(torch.abs(img[1:, :] - img[:-1, :]), dim=2) > thresh\n",
    "\n",
    "    # Calculate the difference between each pixel and the one to its right\n",
    "    rightDif = torch.sum(torch.abs(img[:, 1:] - img[:, :-1]), dim=2) > thresh\n",
    "    #print(torch.abs(img[:, 1:] - img[:, :-1]))\n",
    "    # Initialize a binary image with False values\n",
    "    split = torch.zeros_like(img[:, :, 0], dtype=torch.bool)\n",
    "\n",
    "    # Set the boundary pixels to True based on the differences computed\n",
    "    split[:-1, :] |= downDif\n",
    "    split[:, :-1] |= rightDif\n",
    "\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a3cc6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T11:59:44.309827Z",
     "iopub.status.busy": "2023-09-23T11:59:44.309609Z",
     "iopub.status.idle": "2023-09-23T11:59:44.330779Z",
     "shell.execute_reply": "2023-09-23T11:59:44.330145Z",
     "shell.execute_reply.started": "2023-09-23T11:59:44.309762Z"
    }
   },
   "outputs": [],
   "source": [
    "#models\n",
    "class DeepGreyResnet101(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom deep neural network architecture based on ResNet101, adapted for grayscale (single-channel) input images.\n",
    "\n",
    "    Attributes:\n",
    "        resnet (nn.Sequential): Custom ResNet101 backbone.\n",
    "        avgpool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
    "        flatten (nn.Flatten): Layer to flatten the output tensor.\n",
    "        dropoutn (nn.Dropout): Dropout layers, up to 3.\n",
    "        fcn (nn.Linear): Fully connected layers, up to 4.\n",
    "        activation (nn.LeakyReLU): Activation function layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, p_conv=0.1, p_lin = 0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of classes for the final output.\n",
    "            p_conv (float, optional): Dropout probability for convolutional layers. Defaults to 0.1.\n",
    "            p_lin (float, optional): Dropout probability for linear layers. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #Define custom ResNet architecture for single-channel input by replacing the first layer\n",
    "        layers = list(resnet101(pretrained=True).children())[:-2]\n",
    "        in_channels = 1\n",
    "        first_conv_layer = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        first_conv_layer.weight.data = layers[0].weight.sum(1, keepdim=True)\n",
    "        self.resnet = nn.Sequential(first_conv_layer)\n",
    "        \n",
    "        #Add dropout and activation to convolutional layers\n",
    "        for layer in layers[1:]:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.resnet.add_module(\"conv_dropout\", nn.Dropout2d(p_conv))\n",
    "            self.resnet.add_module(str(len(self.resnet)), layer)\n",
    "\n",
    "        #Define linear section of the model\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p_lin)\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.dropout2 = nn.Dropout(p_lin)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout3 = nn.Dropout(p_lin)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, n_classes)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = self.resnet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "#models\n",
    "class DeepGreyResnet152(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom deep neural network architecture based on ResNet152, adapted for grayscale (single-channel) input images.\n",
    "\n",
    "    Attributes:\n",
    "        resnet (nn.Sequential): Custom ResNet101 backbone.\n",
    "        avgpool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
    "        flatten (nn.Flatten): Layer to flatten the output tensor.\n",
    "        dropoutn (nn.Dropout): Dropout layers, up to 3.\n",
    "        fcn (nn.Linear): Fully connected layers, up to 4.\n",
    "        activation (nn.LeakyReLU): Activation function layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, p_conv=0.1, p_lin = 0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of classes for the final output.\n",
    "            p_conv (float, optional): Dropout probability for convolutional layers. Defaults to 0.1.\n",
    "            p_lin (float, optional): Dropout probability for linear layers. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #Define custom ResNet architecture for single-channel input by replacing the first layer\n",
    "        self.n_classes = n_classes\n",
    "        layers = list(resnet152(pretrained=True).children())[:-2]\n",
    "        in_channels = 1\n",
    "        first_conv_layer = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        first_conv_layer.weight.data = layers[0].weight.sum(1, keepdim=True)\n",
    "        self.resnet = nn.Sequential(first_conv_layer)\n",
    "        \n",
    "        #Add dropout and activation to convolutional layers\n",
    "        for layer in layers[1:]:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.resnet.add_module(\"conv_dropout\", nn.Dropout2d(p_conv))\n",
    "            self.resnet.add_module(str(len(self.resnet)), layer)\n",
    "\n",
    "        #Define linear section of the model\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p_lin)\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.dropout2 = nn.Dropout(p_lin)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.dropout3 = nn.Dropout(p_lin)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, n_classes)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = self.resnet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        \"\"\"if self.n_classes > 4:\n",
    "            ind = self.n_classes - 4\n",
    "            x[:, -ind:] = F.softmax(x[:, -ind:], dim=1)\"\"\"\n",
    "        return x\n",
    "    \n",
    "\n",
    "#models\n",
    "class DeepGreyResnet50(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom deep neural network architecture based on ResNet152, adapted for grayscale (single-channel) input images.\n",
    "\n",
    "    Attributes:\n",
    "        resnet (nn.Sequential): Custom ResNet101 backbone.\n",
    "        avgpool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
    "        flatten (nn.Flatten): Layer to flatten the output tensor.\n",
    "        dropoutn (nn.Dropout): Dropout layers, up to 3.\n",
    "        fcn (nn.Linear): Fully connected layers, up to 4.\n",
    "        activation (nn.LeakyReLU): Activation function layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, p_conv=0.1, p_lin = 0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of classes for the final output.\n",
    "            p_conv (float, optional): Dropout probability for convolutional layers. Defaults to 0.1.\n",
    "            p_lin (float, optional): Dropout probability for linear layers. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #Define custom ResNet architecture for single-channel input by replacing the first layer\n",
    "        self.n_classes = n_classes\n",
    "        layers = list(resnet50(pretrained=True).children())[:-2]\n",
    "        in_channels = 1\n",
    "        first_conv_layer = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        first_conv_layer.weight.data = layers[0].weight.sum(1, keepdim=True)\n",
    "        self.resnet = nn.Sequential(first_conv_layer)\n",
    "        \n",
    "        #Add dropout and activation to convolutional layers\n",
    "        for layer in layers[1:]:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.resnet.add_module(\"conv_dropout\", nn.Dropout2d(p_conv))\n",
    "            self.resnet.add_module(str(len(self.resnet)), layer)\n",
    "\n",
    "        #Define linear section of the model\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout1 = nn.Dropout(p_lin)\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.dropout2 = nn.Dropout(p_lin)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.dropout3 = nn.Dropout(p_lin)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, n_classes)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = self.resnet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        \"\"\"if self.n_classes > 4:\n",
    "            ind = self.n_classes - 4\n",
    "            x[:, -ind:] = F.softmax(x[:, -ind:], dim=1)\"\"\"\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class oneGreyResnet50(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom deep neural network architecture based on ResNet152, adapted for grayscale (single-channel) input images.\n",
    "\n",
    "    Attributes:\n",
    "        resnet (nn.Sequential): Custom ResNet101 backbone.\n",
    "        avgpool (nn.AdaptiveAvgPool2d): Adaptive average pooling layer.\n",
    "        flatten (nn.Flatten): Layer to flatten the output tensor.\n",
    "        dropoutn (nn.Dropout): Dropout layers, up to 3.\n",
    "        fcn (nn.Linear): Fully connected layers, up to 4.\n",
    "        activation (nn.LeakyReLU): Activation function layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, p_conv=0.1, p_lin = 0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of classes for the final output.\n",
    "            p_conv (float, optional): Dropout probability for convolutional layers. Defaults to 0.1.\n",
    "            p_lin (float, optional): Dropout probability for linear layers. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #Define custom ResNet architecture for single-channel input by replacing the first layer\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        layers = list(resnet50(pretrained=True).children())[:-2]\n",
    "        \n",
    "        in_channels = 1\n",
    "        first_conv_layer = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        first_conv_layer.weight.data = layers[0].weight.sum(1, keepdim=True)\n",
    "        \n",
    "        self.resnet = nn.Sequential(first_conv_layer)\n",
    "        \n",
    "        #Add dropout and activation to convolutional layers\n",
    "        for layer in layers[1:]:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.resnet.add_module(\"conv_dropout\", nn.Dropout2d(p_conv))\n",
    "            self.resnet.add_module(str(len(self.resnet)), layer)\n",
    "\n",
    "        #Define linear section of the model\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1s = nn.Linear(2048, 2)\n",
    "        \n",
    "        self.fc1q = nn.Linear(2048, 1024)\n",
    "        self.fc2q = nn.Linear(1024, 512)\n",
    "        self.fc3q = nn.Linear(512, 256)\n",
    "        self.fc4q = nn.Linear(256, 4)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        self.final = False\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = self.resnet(x)\n",
    "        #print(x.shape)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        xs = self.fc1s(x)\n",
    "        \n",
    "        if self.final:\n",
    "            xs = self.final(xs)\n",
    "        #xs = self.activation(xs)\n",
    "        #xs = self.fc2s(xs)\n",
    "        #xs = self.activation(xs)\n",
    "        #xs = self.fc3s(xs)\n",
    "        #xs = self.activation(xs)\n",
    "        #xs = self.fc4s(xs)\n",
    "        #xs = self.catActivation(xs)\n",
    "        \n",
    "        xq = self.fc1q(x)\n",
    "        xq = self.activation(xq)\n",
    "        xq = self.fc2q(xq)\n",
    "        xq = self.activation(xq)\n",
    "        xq = self.fc3q(xq)\n",
    "        xq = self.activation(xq)\n",
    "        xq = self.fc4q(xq)\n",
    "        \n",
    "        x = torch.cat([xq, xs], dim = -1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ccd8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T11:59:44.332041Z",
     "iopub.status.busy": "2023-09-23T11:59:44.331656Z",
     "iopub.status.idle": "2023-09-23T11:59:44.344025Z",
     "shell.execute_reply": "2023-09-23T11:59:44.343402Z",
     "shell.execute_reply.started": "2023-09-23T11:59:44.332019Z"
    }
   },
   "outputs": [],
   "source": [
    "#losses\n",
    "\n",
    "def l1Simple(q1, q2):\n",
    "    \"\"\"\n",
    "    Compute the L1 loss between the closest point in q2 to a reference and q1.\n",
    "    \n",
    "    Args:\n",
    "        q1 (torch.Tensor): Target tensor.\n",
    "        q2 (torch.Tensor): Labels tensor.\n",
    "        \n",
    "    Returns:\n",
    "        loss (torch.Tensor): Computed L1 loss.\n",
    "    \"\"\"\n",
    "        \n",
    "    ref = torch.tensor([1, 1e-8, 1e-16, 1e-24]).cuda()\n",
    "    ref = ref / torch.norm(ref, dim=-1, keepdim=True)\n",
    "    \n",
    "    diff = ref - q2\n",
    "    distance = torch.norm(diff, dim=-1)\n",
    "    ind = torch.argmin(distance, dim=1)\n",
    "    closest = q2[torch.arange(q1.shape[0]), ind, :]\n",
    "    loss = F.l1_loss(closest, q1)\n",
    "    return loss\n",
    "\n",
    "def l1Sym(q1, q2):\n",
    "    \"\"\"\n",
    "    Compute the L1 loss between the closest point in q2 to q1.\n",
    "    \n",
    "    Args:\n",
    "        q1 (torch.Tensor): Target tensor.\n",
    "        q2 (torch.Tensor): Labels tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Computed L1 loss.\n",
    "    \"\"\"\n",
    "    ref = q1 / torch.norm(q1, dim=-1, keepdim=True)\n",
    "    diff = ref.unsqueeze(1) - q2\n",
    "    distance = torch.norm(diff, dim=-1)\n",
    "    ind = torch.argmin(distance, dim=1)\n",
    "    closest = q2[torch.arange(q1.shape[0]), ind, :]\n",
    "    loss = F.l1_loss(closest, q1)\n",
    "    return loss\n",
    "\n",
    "def trueRotDistance(q1, q2):\n",
    "\n",
    "    q1Norm = q1 / torch.norm(q1, dim=-1, keepdim=True)\n",
    "    q2Norm = q2 / torch.norm(q2, dim=-1, keepdim=True)\n",
    "\n",
    "    dotProduct = torch.sum(q1Norm.unsqueeze(1) * q2Norm, dim=-1)\n",
    "    \n",
    "    dotProduct = torch.clamp(dotProduct, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "    # Compute the geodesic distance between the quaternions\n",
    "    distance = torch.acos(dotProduct) * 360 / 3.14159\n",
    "    closestTheta, _ = torch.min(distance, dim = 1)\n",
    "    return torch.mean(closestTheta)\n",
    "\n",
    "def mixedLoss(q1, q2):\n",
    "    \"\"\"\n",
    "    Compute a mixed loss using geodesic distance and L1 symmetrical loss.\n",
    "    \n",
    "    Args:\n",
    "        q1 (torch.Tensor): Target tensor.\n",
    "        q2 (torch.Tensor): Labels tensor.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Computed mixed loss.\n",
    "    \"\"\"\n",
    "    return trueRotDistance(q1, q2) + 10 * l1Sym(q1, q2)\n",
    "\n",
    "import numpy\n",
    "import numpy.matlib as npm\n",
    "\n",
    "def avgRotDistance(q1, q2):\n",
    "\n",
    "    q1Norm = q1 / torch.norm(q1, dim=-1, keepdim=True)\n",
    "    q2Norm = q2 / torch.norm(q2, dim=-1, keepdim=True)\n",
    "    \n",
    "    q1Norm = q1Norm.unsqueeze(1)\n",
    "    \n",
    "    dotProduct = torch.abs(torch.sum(q1Norm * q2Norm, dim=-1))\n",
    "\n",
    "\n",
    "    distance = torch.acos(dotProduct) * 360 / 3.14159\n",
    "    return torch.mean(distance)\n",
    "\n",
    "def averageQuaternions(Q):\n",
    "    \"\"\"\n",
    "    Compute the average quaternion of an array of quaternions.\n",
    "    \n",
    "    This function calculates the average by determining the eigenvector \n",
    "    associated with the largest eigenvalue of the quaternion cross-product matrix.\n",
    "    Reference: https://github.com/christophhagen/averaging-quaternions/blob/master/averageQuaternions.py\n",
    "    \n",
    "    Args:\n",
    "        Q (numpy.ndarray): An Mx4 matrix, where M is the number of quaternions and each quaternion is represented by a row of 4 elements.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: A quaternion representing the average.\n",
    "    \"\"\"\n",
    "\n",
    "    M = Q.shape[0]\n",
    "    A = npm.zeros(shape=(4,4))\n",
    "\n",
    "    for i in range(0,M):\n",
    "        q = Q[i,:]\n",
    "        # multiply q with its transposed version q' and add A\n",
    "        A = numpy.outer(q,q) + A\n",
    "\n",
    "    # scale\n",
    "    A = (1.0/M)*A\n",
    "    # compute eigenvalues and -vectors\n",
    "    eigenValues, eigenVectors = numpy.linalg.eig(A)\n",
    "    # Sort by largest eigenvalue\n",
    "    eigenVectors = eigenVectors[:,eigenValues.argsort()[::-1]]\n",
    "    # return the real part of the largest eigenvector (has only real part)\n",
    "    return numpy.real(eigenVectors[:,0].A1)\n",
    "\n",
    "def quatL1Simple(q1, q2):\n",
    "    ref = torch.tensor([1, 1e-8, 1e-16, 1e-24]).cuda()\n",
    "    ref = ref / torch.norm(ref, dim=-1, keepdim=True)\n",
    "    \n",
    "    diff = ref - q2[:, :, :4]\n",
    "    distance = torch.norm(diff, dim=-1)\n",
    "    ind = torch.argmin(distance, dim=1)\n",
    "    closest = q2[torch.arange(q1.shape[0]), ind, :]\n",
    "    loss = F.l1_loss(closest[:, :4], q1[:, :4])\n",
    "    return loss\n",
    "\n",
    "def quatTrueRotDistance(q1, q2):\n",
    "    q1Norm = q1[:, :4] / torch.norm(q1[:, :4], dim=-1, keepdim=True)\n",
    "    q2Norm = q2[:, :, :4]  / torch.norm(q2[:, :, :4] , dim=-1, keepdim=True)\n",
    "\n",
    "    dotProduct = torch.sum(q1Norm.unsqueeze(1) * q2Norm, dim=-1)\n",
    "    \n",
    "    dotProduct = torch.clamp(dotProduct, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "    # Compute the geodesic distance between the quaternions\n",
    "    distance = torch.acos(dotProduct) * 360 / 3.14159\n",
    "    #print(distance.shape)\n",
    "    closestTheta, _ = torch.min(distance, dim = 1)\n",
    "    #print(closestTheta.shape)\n",
    "    return torch.mean(closestTheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12efe374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T11:59:44.345843Z",
     "iopub.status.busy": "2023-09-23T11:59:44.345254Z",
     "iopub.status.idle": "2023-09-23T11:59:44.373005Z",
     "shell.execute_reply": "2023-09-23T11:59:44.372321Z",
     "shell.execute_reply.started": "2023-09-23T11:59:44.345843Z"
    }
   },
   "outputs": [],
   "source": [
    "#transforms\n",
    "\n",
    "class zoom:\n",
    "    def __init__(self, zoomFactor = 2.0, imgSize = 144):\n",
    "        self.zoomFactor = zoomFactor\n",
    "        self.imgSize = imgSize\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        initialImg = img\n",
    "        \n",
    "        zoomFactor =  (self.zoomFactor - 1) * random.random() + 1\n",
    "        zoomSize = max(self.imgSize, int(zoomFactor * self.imgSize))\n",
    "        img = cv2.resize(img[0, :, :].numpy(), (zoomSize, zoomSize), interpolation = cv2.INTER_LANCZOS4)\n",
    "        edges = int((zoomSize - self.imgSize) / 2)\n",
    "        img = img[edges: -edges, edges: -edges]\n",
    "        img = img[:self.imgSize, :self.imgSize]\n",
    "        if img.shape[1] == self.imgSize:\n",
    "            return torch.Tensor(img).unsqueeze(0)\n",
    "        else:\n",
    "            return initialImg\n",
    "        \n",
    "def createCircularArray(rows, cols, center, radius):\n",
    "    \"\"\"\n",
    "    Creates a 2D binary array of given dimensions where cells within a given radius\n",
    "    from a specified center are set to 1, and others are set to 0.\n",
    "    \n",
    "    Parameters:\n",
    "        rows (int): Number of rows for the 2D array.\n",
    "        cols (int): Number of columns for the 2D array.\n",
    "        center (tuple): Coordinates of the center.\n",
    "        radius (int): Radius for the circle.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: 2D binary array.\n",
    "    \"\"\"\n",
    "    #Initialize a 2D array with zeros of specified rows and cols.\n",
    "    arr = np.zeros((rows, cols), dtype=int)\n",
    "    \n",
    "    #Iterate through each cell in the 2D array.\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            #Compute the distance from the current cell to the center.\n",
    "            distance = np.sqrt((i - center[0])**2 + (j - center[1])**2)\n",
    "            \n",
    "            #If the distance is less than or equal to the given radius, set that cell to 1.\n",
    "            if distance <= radius:\n",
    "                arr[i, j] = 1\n",
    " \n",
    "    return arr\n",
    "\n",
    "class AddNoise:\n",
    "    \"\"\"\n",
    "    Callable class that adds random noise to an image.\n",
    "    \n",
    "    Attributes:\n",
    "        noise_factor (float): Measure of how much noise to introduce.\n",
    "    \"\"\"\n",
    "    def __init__(self, noiseFactor):\n",
    "        #Set the noise factor (a measure of how much noise to introduce).\n",
    "        self.noiseFactor = noiseFactor\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Add noise to the given image based on the noise factor.\n",
    "        \n",
    "        Args:\n",
    "        - img (torch.Tensor): Input image.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Noisy image.\n",
    "        \"\"\"\n",
    "\n",
    "        #Add noise with a 50 percent chance\n",
    "        chance = random.randint(1, 10) \n",
    "        if chance >= 6:\n",
    "            return img\n",
    "        \n",
    "        #Add random noise to the image based on the noise factor.\n",
    "        noisyImg = img + random.uniform(0, self.noiseFactor) * torch.randn(*img.shape)\n",
    "        return noisyImg\n",
    "\n",
    "class CircularCrop:\n",
    "    \"\"\"\n",
    "    Callable class that crops an image in a circular shape.\n",
    "    \n",
    "    Attributes:\n",
    "        centre (tuple): Coordinates of the center for cropping.\n",
    "        rand (bool): Flag to determine if random cropping is enabled.\n",
    "        radius (int): Radius for the circular crop.\n",
    "        size (int): Size of the side for cropping.\n",
    "        circular_array (numpy.ndarray): 2D binary array for cropping.\n",
    "    \"\"\"\n",
    "    def __init__(self, radius = 30, size = 60, rand = False):\n",
    "        self.centre = (size // 2, size // 2) \n",
    "        self.rand = rand\n",
    "        self.radius = radius\n",
    "        self.size = size\n",
    "        \n",
    "        #Generate a circular array of ones inside the specified radius.\n",
    "        circularArray = createCircularArray(self.size, self.size, self.centre, self.radius)\n",
    "        #Expand the dimensions of the circular array.\n",
    "        self.circularArray = np.expand_dims(circularArray, 0)\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Crop the given image in a circular shape.\n",
    "        \n",
    "        Args:\n",
    "            img (torch.Tensor): Input image.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Cropped image.\n",
    "        \"\"\"\n",
    "\n",
    "        #If random cropping is enabled:\n",
    "        if self.rand:\n",
    "            #Generate a random radius between the specified radius and 30.\n",
    "            randRadius = random.randint(self.radius, self.size / 2)\n",
    "\n",
    "            #Recreate the circular array based on the new radius.\n",
    "            self.circularArray = createCircularArray(self.size, self.size, self.centre, randRadius)\n",
    "            self.circularArray = np.expand_dims(self.circularArray, 0)\n",
    "\n",
    "        #Multiply the image with the circular array to get the cropped area.\n",
    "        data = img * self.circularArray\n",
    "        return data.half()\n",
    "    \n",
    "class Bright:\n",
    "    \"\"\"\n",
    "    Brightens an image by applying a brightness gradient at a random offset.\n",
    "    \n",
    "    Attributes:\n",
    "        brightFactor (float): Factor to control the intensity of brightness.\n",
    "        size (int): Size of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, brightFactor, size = 60):\n",
    "        self.brightFactor = brightFactor\n",
    "        self.size = size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        chance = random.randint(1, 10) \n",
    "        if chance >= 6:\n",
    "            return img\n",
    "        centerOffset = np.random.randint(-self.size  // 4, self.size  // 4, size=2)\n",
    "        y, x = np.mgrid[-self.size  // 2 + centerOffset[0]:self.size  // 2 + centerOffset[0], \n",
    "                        -self.size  // 2 + centerOffset[1]:self.size  // 2 + centerOffset[1]]\n",
    "\n",
    "        radius = np.sqrt(x ** 2 + y ** 2)\n",
    "        gradient = np.exp(-2e-3 * random.uniform(0.1, 4) * radius)\n",
    "\n",
    "        normalizedGradient = gradient / gradient.max()\n",
    "        return img + normalizedGradient * random.uniform(-self.brightFactor, self.brightFactor)\n",
    "\n",
    "class Normalise: \n",
    "    \"\"\"\n",
    "    Normalises an image so its values range between 0 and 1.\n",
    "    \"\"\"\n",
    "    def __call__(self, img):\n",
    "        return (img - torch.min(img)) / (torch.max(img) - torch.min(img))\n",
    "\n",
    "class randomBlur:\n",
    "    \"\"\"\n",
    "    Applies a Gaussian blur to an image with a random kernel size.\n",
    "    \n",
    "    Attributes:\n",
    "        maxKernelSize (int): Maximum size for the Gaussian blur kernel.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxKernelSize = 5):\n",
    "        self.maxKernelSize = maxKernelSize\n",
    "\n",
    "    def __call__(self, img): \n",
    "        chance = random.randint(1, 10) \n",
    "        if chance >= 6:\n",
    "            return img\n",
    "        \n",
    "        else:\n",
    "            img = img.to(torch.float32)\n",
    "            kernelSize = random.randint(1, self.maxKernelSize) \n",
    "            \n",
    "            #Ensure an odd kernel size\n",
    "            if kernelSize % 2 == 0:\n",
    "                kernelSize -= 1  \n",
    "\n",
    "            blurTransform = transforms.GaussianBlur(kernelSize, sigma=(0.1, 2.0))\n",
    "            blurImg = blurTransform(img)\n",
    "            return blurImg  \n",
    "    \n",
    "class BrightNoise:\n",
    "    \"\"\"\n",
    "    Adds a random bright noise within a circular region in the image.\n",
    "    \n",
    "    Attributes:\n",
    "        bright_factor (float): Intensity factor of brightness noise.\n",
    "        max_radius (int): Maximum possible radius for the noise circle.\n",
    "    \"\"\"\n",
    "    def __init__(self, bright_factor, max_radius = 30):\n",
    "        self.bright_factor = bright_factor\n",
    "        self.max_radius = max_radius\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        chance = random.randint(1, 10) \n",
    "        if chance >= 6:\n",
    "            return img\n",
    "        h, w = img.shape[1], img.shape[2]\n",
    "        circle_center = (random.randint(self.max_radius, w - self.max_radius),\n",
    "                         random.randint(self.max_radius, h - self.max_radius))\n",
    "        circle_radius = random.randint(5,  self.max_radius)\n",
    "        \n",
    "        y, x = np.ogrid[:h, :w]\n",
    "        \n",
    "        mask = ((x - circle_center[0]) ** 2 + (y - circle_center[1]) ** 2) >= circle_radius ** 2\n",
    "        noise = torch.Tensor(np.random.normal(size=(1, h, w))) * mask * self.bright_factor\n",
    "        return noise + img\n",
    "    \n",
    "class addTo:\n",
    "    \"\"\"\n",
    "    Increases or decreases the pixel values of an image by a random amount.\n",
    "    \n",
    "    Attributes:\n",
    "        max_add (float): Maximum value to add or subtract from the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_add = 0.25):\n",
    "        self.max_add = max_add\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return img + random.uniform(-self.max_add, self.max_add)\n",
    "\n",
    "class scaleBy: \n",
    "    \"\"\"\n",
    "    Scales the pixel values of an image by a random factor.\n",
    "    \n",
    "    Attributes:\n",
    "        maxScale (float): Maximum scaling factor.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxScale = 0.5):\n",
    "        self.maxScale = maxScale\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        scaleFactor = random.uniform(1 - self.maxScale, 1 / (1 - self.maxScale))\n",
    "        return img * scaleFactor\n",
    "    \n",
    "class mixedTransform:\n",
    "    \"\"\"\n",
    "    Applies a series of transformations to an image, converting it to a PIL image \n",
    "    and then back to a tensor after transformations.\n",
    "    \n",
    "    Attributes:\n",
    "        transformList (list): List of transforms to apply to the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformList):\n",
    "        self.transformList = transformList\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        mixedPilImage = to_pil_image(img)\n",
    "        \n",
    "        for transform in self.transformList:\n",
    "            mixedPilImage = transform(mixedPilImage)\n",
    "         \n",
    "        mixed_tensor = transforms.ToTensor()(mixedPilImage)  \n",
    "        return mixed_tensor\n",
    "\n",
    "class setNaNToZero:\n",
    "    \"\"\"\n",
    "    Transform that sets NaN values in a tensor to a specific replacement value.\n",
    "    \n",
    "    Attributes:\n",
    "        replacementValue (float): Value to replace NaN values with. Defaults to 0.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, replacementValue = 0.0):\n",
    "        \"\"\"Initialize setNaNToZero class with the given replacement value.\"\"\"\n",
    "        self.replacementValue = replacementValue\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Call method to process the image tensor and replace NaN values.\n",
    "        \n",
    "        Args:\n",
    "            img (torch.Tensor): Input image tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Processed image tensor.\n",
    "        \"\"\"\n",
    "        # Check for NaN values in the image tensor\n",
    "        mask = torch.isnan(img)\n",
    "        \n",
    "        # Replace NaN values with the specified replacement value\n",
    "        img[mask] = self.replacementValue\n",
    "        \n",
    "        return img.float()\n",
    "    \n",
    "\n",
    "class StretchAndCropTransform:\n",
    "    \"\"\"\n",
    "    Transform that applies random stretching to a tensor in both x and y axes, \n",
    "    then resizes to a standard size.\n",
    "    \n",
    "    Attributes:\n",
    "        xStretch (int): Maximum factor to stretch in the x-axis.\n",
    "        yStretch (int): Maximum factor to stretch in the y-axis.\n",
    "        standard_size (int): Size to resize the image after stretching. Defaults to 144.\n",
    "    \"\"\"\n",
    "    def __init__(self, xStretch = 2, yStretch = 2, standard_size = 144):\n",
    "        \"\"\"Initialize StretchAndCropTransform class with given stretch factors and standard size.\"\"\"\n",
    "        self.xStretch = xStretch\n",
    "        self.yStretch = yStretch\n",
    "        self.standard_size = standard_size\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Call method to process the tensor and apply random stretching and resizing.\n",
    "        \n",
    "        Args:\n",
    "            tensor (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor.\n",
    "        \"\"\"\n",
    "        chance = random.randint(1, 10) \n",
    "        if chance >= 6:\n",
    "            return tensor\n",
    "        originalWidth, originalHeight = tensor.shape[1], tensor.shape[2]\n",
    "        \n",
    "        xStretch = random.uniform(1,  self.xStretch) * originalWidth\n",
    "        yStretch = random.uniform(1,  self.yStretch) * originalHeight\n",
    "        \n",
    "        padHeight = int(max((yStretch - xStretch), 0) / 2)\n",
    "        padWidth= int(max((xStretch - yStretch), 0) / 2)\n",
    "        \n",
    "        padded = torch.nn.functional.pad(tensor.unsqueeze(0), (padWidth, padWidth, padHeight, padHeight), value=0)\n",
    "                                         \n",
    "        resized = F.interpolate(padded, self.standard_size, mode='bilinear', align_corners=True)\n",
    "        resized = resized.squeeze(0)\n",
    "\n",
    "        return resized\n",
    "\n",
    "\n",
    "class CircularCrop:\n",
    "    \"\"\"\n",
    "    Transform that crops an image tensor into a circle.\n",
    "    \n",
    "    Attributes:\n",
    "        centre (tuple): Centre of the circle to crop.\n",
    "        radius (int): Radius of the circle.\n",
    "        size (int): Size of the circular mask.\n",
    "        circular_array (torch.Tensor): The mask to apply circular cropping.\n",
    "    \"\"\"\n",
    "    def __init__(self, radius = 30, size = 60):\n",
    "        \"\"\"Initialize CircularCrop class with given radius and size.\"\"\"\n",
    "        self.centre = (size // 2, size // 2) \n",
    "        self.radius = radius\n",
    "        self.size = size\n",
    "        \n",
    "        self.circular_array = createCircularArray(self.size, self.size, self.centre, self.radius)\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Call method to process the tensor and apply the circular crop.\n",
    "        \n",
    "        Args:\n",
    "            img (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            data (torch.Tensor): Cropped tensor.\n",
    "        \"\"\"\n",
    "        data = img * self.circular_array\n",
    "        return data.float()\n",
    "\n",
    "class RandCircularCrop:\n",
    "    \"\"\"\n",
    "    Transform that crops an image tensor into a circle with random radius.\n",
    "    \n",
    "    Attributes:\n",
    "        centre (tuple): Centre of the circle to crop.\n",
    "        radius (int): Base radius for the circle.\n",
    "        size (int): Size of the circular mask.\n",
    "        circular_array_list (list of torch.Tensor): List of masks for different radii.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, radius = 30, size = 60):\n",
    "        self.centre = (size // 2, size // 2) \n",
    "        self.radius = radius\n",
    "        self.size = size\n",
    "        self.circular_array_list = []\n",
    "        \n",
    "        for i in range(radius, size // 2):\n",
    "            circular_array = createCircularArray(self.size, self.size, self.centre, i)\n",
    "            self.circular_array_list.append(circular_array)\n",
    "            \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Call method to apply a circular crop of random radius.\n",
    "        \n",
    "        Args:\n",
    "            img (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            data (torch.Tensor): Cropped tensor.\n",
    "        \"\"\"\n",
    "        randRadius = random.randint(0, self.size // 2 - self.radius - 1)\n",
    "\n",
    "        data = img * self.circular_array_list[randRadius]\n",
    "        return data.float()\n",
    "\n",
    "def randomTransform(transform, p=0.5):\n",
    "    \"\"\"\n",
    "    A function to make a given transform apply randomly with a specified probability.\n",
    "    \n",
    "    Args:\n",
    "        transform: The transformation function to be applied.\n",
    "        p (float, optional): Probability of applying the transform. Default is 0.5.\n",
    "    \n",
    "    Returns:\n",
    "        A randomly applied transformation object.\n",
    "    \"\"\"\n",
    "    return transforms.RandomApply([transform], p=p)\n",
    "\n",
    "def noiseReductionMedian(image, filterSize):\n",
    "    \"\"\"\n",
    "    Apply a median filter to an image for noise reduction.\n",
    "    \n",
    "    Parameters:\n",
    "        image (torch tensor): Input image that noise reduction will be applied to.\n",
    "        filterSize (int): Size of the median filter to be applied.\n",
    "    \n",
    "    Returns:\n",
    "        denoisedImage: Image after noise reduction using median filtering.\n",
    "    \"\"\"\n",
    "    denoisedImage = median_filter(image, size = filterSize)\n",
    "    return denoisedImage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
